from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import requests
import os

app = FastAPI()

# --- CORS SETUP ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") 

# --- PROJECTS DATA ---
projects = [
    { "id": 1, "title": "SMOKIO", "desc": "Next.js & Three.js", "tech": "NEXT.JS / THREE.JS", "video": "/videos/smokio-3d-site.mp4", "link": "https://taupe-axolotl-9a3639.netlify.app/" },
    { "id": 2, "title": "ERP SYSTEM", "desc": "Factory management system.", "tech": "LARAVEL / VUE.JS", "video": "/videos/erp.mp4", "link": "#" },
    { "id": 3, "title": "EFRAME", "desc": "Photo framing service.", "tech": "PYTHON / REACT", "video": "/videos/eframe.mp4", "link": "https://eframe.store" }
]

@app.get("/")
def read_root():
    return {"message": "Ravindu's Auto-Healing API is Online! üõ†Ô∏è"}

@app.get("/projects")
def get_projects():
    return projects

# --- SMART CHAT LOGIC ---
system_instruction = """
You are Ravindu's AI. Answer simply and shortly.
- "Who is Arjun?": "Arjun is the Boss! Eframe Owner."
- "Who is Nimna?": "Nimna is the Marketing Genius! (Track Ela Kollek)."
"""

class ChatRequest(BaseModel):
    message: str

def get_working_model():
    """Google ‡∂ë‡∂ö‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ê‡∂© ‡∂ö‡∂ª‡∂± ‡∂∏‡∑ú‡∂©‡∂Ω‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂â‡∂Ω‡∑ä‡∂Ω‡∂ú‡∂±‡∑ä‡∂±‡∑Ä‡∑è"""
    try:
        url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GEMINI_API_KEY}"
        response = requests.get(url)
        data = response.json()
        
        if "models" in data:
            for m in data["models"]:
                # 'generateContent' ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑ä ‡∂∏‡∑ú‡∂©‡∂Ω‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∑Ñ‡∑ú‡∂∫‡∂∏‡∑î
                if "generateContent" in m.get("supportedGenerationMethods", []):
                    model_name = m["name"].split("/")[-1] # "models/gemini-pro" -> "gemini-pro"
                    print(f"‚úÖ Found Working Model: {model_name}")
                    return model_name
    except:
        pass
    return "gemini-pro" # ‡∂∂‡∑ê‡∂ª‡∑í‡∂∏ ‡∑Ä‡∑î‡∂±‡∑ú‡∂≠‡∑ä ‡∂∏‡∑ö‡∂ö ‡∂Ø‡∑è‡∂±‡∑Ä‡∑è

@app.post("/chat")
def chat(request: ChatRequest):
    if not GEMINI_API_KEY:
        return {"reply": "Server Error: No API Key."}

    # 1. ‡∂∏‡∑î‡∂Ω‡∑í‡∂±‡∑ä‡∂∏ Default ‡∂ë‡∂ö ‡∂ß‡∑ä‚Äç‡∂ª‡∂∫‡∑í ‡∂ö‡∂ª‡∂∏‡∑î
    current_model = "gemini-1.5-flash"
    
    full_prompt = f"{system_instruction}\nUser: {request.message}\nAI:"
    payload = {"contents": [{"parts": [{"text": full_prompt}]}]}
    
    # ‡∂¥‡∑Ö‡∑Ä‡∑ô‡∂±‡∑í ‡∂ã‡∂≠‡∑ä‡∑É‡∑è‡∑Ñ‡∂∫
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{current_model}:generateContent?key={GEMINI_API_KEY}"
    response = requests.post(url, json=payload, headers={"Content-Type": "application/json"})
    data = response.json()

    # 2. Error ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂Ü‡∑Ä‡∑ú‡∂≠‡∑ä, Auto-Fix ‡∂¥‡∂ß‡∂±‡∑ä ‡∂ú‡∂±‡∑ä‡∂±‡∑Ä‡∑è
    if "error" in data:
        print(f"‚ö†Ô∏è Model {current_model} failed. Finding a new one...")
        
        # ‡∂Ö‡∂Ω‡∑î‡∂≠‡∑ä ‡∑Ä‡∑ê‡∂© ‡∂ö‡∂ª‡∂± ‡∂∏‡∑ú‡∂©‡∂Ω‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∑Ñ‡∑ú‡∂∫‡∑è‡∂ú‡∂±‡∑ä‡∂±‡∑Ä‡∑è
        new_model = get_working_model()
        print(f"üîÑ Switching to: {new_model}")
        
        # ‡∂Ö‡∂Ω‡∑î‡∂≠‡∑ä ‡∂∏‡∑ú‡∂©‡∂Ω‡∑ä ‡∂ë‡∂ö‡∑ô‡∂±‡∑ä ‡∂Ü‡∂∫‡∑ö ‡∂ß‡∑ä‚Äç‡∂ª‡∂∫‡∑í ‡∂ö‡∂ª‡∂±‡∑Ä‡∑è
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{new_model}:generateContent?key={GEMINI_API_KEY}"
        response = requests.post(url, json=payload, headers={"Content-Type": "application/json"})
        data = response.json()

    # 3. ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂µ‡∂Ω‡∂∫ ‡∂∫‡∑Ä‡∂±‡∑Ä‡∑è
    if "candidates" in data:
        return {"reply": data["candidates"][0]["content"]["parts"][0]["text"]}
    else:
        # ‡∂≠‡∑è‡∂∏ Error ‡∂±‡∂∏‡∑ä, ‡∂í‡∂ö ‡∂ö‡∑ô‡∂Ω‡∑í‡∂±‡∑ä‡∂∏ ‡∂∫‡∑Ä‡∂±‡∑Ä‡∑è (‡∂ë‡∂≠‡∂ö‡∑ú‡∂ß ‡∂Ö‡∂¥‡∑í‡∂ß ‡∂¥‡∑ö‡∂±‡∑Ä‡∑è ‡∂∏‡∑ú‡∂ö‡∂ö‡∑ä‡∂Ø ‡∂Ö‡∑Ä‡∑î‡∂Ω ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è)
        error_msg = data.get('error', {}).get('message', 'Unknown Error')
        return {"reply": f"System Error: {error_msg} (Available models could not be used)."}